{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"jax-unirep: A performant reimplementation of the UniRep model in JAX Welcome to the documentation of jax-unirep . jax-unirep is a reimplementation of the UniRep model, published by Biswas et. al. in 2020, implemented using JAX. Goals The goals of this project are to provide a performant reimplementation with a suite of user-friendly APIs to make usage of the UniRep model in protein machine learning applications more user-friendly.","title":"Home"},{"location":"#jax-unirep-a-performant-reimplementation-of-the-unirep-model-in-jax","text":"Welcome to the documentation of jax-unirep . jax-unirep is a reimplementation of the UniRep model, published by Biswas et. al. in 2020, implemented using JAX.","title":"jax-unirep: A performant reimplementation of the UniRep model in JAX"},{"location":"#goals","text":"The goals of this project are to provide a performant reimplementation with a suite of user-friendly APIs to make usage of the UniRep model in protein machine learning applications more user-friendly.","title":"Goals"},{"location":"advanced/","text":"Advanced Usage APIs that support \"advanced\" tasks are available in jax-unirep . Read on to learn how to use them. Evotuning In the original paper the concept of 'evolutionary finetuning' is introduced, where the pre-trained mLSTM weights get fine-tuned through weight-updates using homolog protein sequences of a given protein of interest as input. This feature is available as well in jax-unirep . Given a set of starter weights for the mLSTM (defaults to the weights from the paper) as well as a set of sequences, the weights get fine-tuned in such a way that test set loss in the 'next-aa prediction task' is minimized. There are two functions with differing levels of control available. The evotune function uses optuna under the hood to automatically find: 1. the optimal number of epochs to train for, and 2. the optimal learning rate, given a set of sequences. The study object will contain all the information about the training process of each trial. evotuned_params will contain the fine-tuned mLSTM and dense weights from the trial with the lowest test set loss. Speed freaks read this! As a heads-up, using evotune is kind of slow, so read on if you're of the impatient kind -- use fit ! If you want to directly fine-tune the weights for a fixed number of epochs while using a fixed learning rate, you should use the fit function instead. The fit function has further customization options, such as different batching strategies. Please see the function docstring for more information. GPU usage The fit function will always default to using a GPU backend if available for the forward and backward passes during training of the LSTM. However, for the calulation of the average loss on the dataset after every epoch, you can decide if the CPU or GPU backend should be used (default is CPU). You can find an example usages of both evotune and fit here . If you want to pass a set of mLSTM and dense weights that were dumped in an earlier run, create params as follows: from jax_unirep.utils import load_params params = load_params ( folderpath = \"path/to/params/folder\" ) If you want to start from randomly initialized mLSTM and dense weights instead: from jax_unirep.evotuning import init_fun from jax.random import PRNGKey _ , params = init_fun ( PRNGKey ( 0 ), input_shape = ( - 1 , 10 )) The weights used in the 10-dimensional embedding of the input sequences always default to the weights from the paper, since they do not get updated during evotuning. End-to-end differentiable models As a user, you might want to write custom \"top models\", such as a linear model on top of the reps, but might want to jointly optimize the UniRep weights with the top model reps. You're in luck! We implemented the mLSTM layers in such a way that they are compatible with jax.experimental.stax . This means that they can easily be plugged into a stax.serial model, e.g. to train both the mLSTM and a top-model at once: from jax.experimental import stax from jax.experimental.stax import Dense , Relu from jax_unirep.layers import mLSTM1900 , mLSTM1900_AvgHidden init_fun , apply_fun = stax . serial ( mLSTM1900 (), mLSTM1900_AvgHidden (), # Add two layers, one dense layer that results in 512-dim activations Dense ( 512 ), Relu (), # And then a linear layer to produce a 1-dim activation Dense ( 1 ) ) Have a look at the documentation and examples for more information about how to implement a model in jax . Sampling new protein sequences When doing protein engineering, one core task is proposing new sequences to order by gene synthesis. jax-unirep provides a number of utility functions inside jax_unirep.sampler that help with this task. Basic sampling The key one to focus on is the sample_one_chain function. This function takes in a starting sequence, and uses Monte Carlo sampling alongside the Metropolis-Hastings criteria to score and rank-order new sequences to try out. The usage pattern is as follows. Firstly, you must have a scoring function defined that takes in a string sequence, and outputs a number. This can be, for example, in the form of a pre-trained machine learning model that you have created. from jax_unirep import get_reps model = SomeSKLearnModel () model . fit ( training_X , training_y ) def scoring_func ( sequence : str ): reps , _ , _ = get_reps ( sequence ) return model . predict ( reps ) Now, we can use MCMC sampling to propose new sequences. from jax_unirep import sample_one_chain starter_seq = \"MKLNEQLJLA\" # can be longer! sampled_sequences = sample_one_chain ( starter_seq , n_steps = 10 , scoring_func = scoring_func ) sampled_seqs_df = pd . DataFrame ( sampled_sequences ) sampled_sequences is a dictionary that can be converted directly into a pandas.DataFrame . In there, every single sequence that was ever sampled is recorded, as well as its score (given by the scoring function) and whether it was accepted by the MCMC sampler or not. (All generated sequences are recorded, just in case there was something good that was rejected!) Parallel sampling If you want to do parallel sampling, you can use any library that does parallel processing. We're going to show you one example using Dask , which happens to be out favourite library for scalable Python! Assuming you have a Dask client object instantiated: client = Client ( ... ) # you'll have to configure this according to your own circumstances starter_seq = \"MKLNEQLJLA\" # can be longer! chain_results_futures = [] for i in range ( 100 ): # sample 100 independent chains chain_results_futures . append ( # Submit tasks to workers client . submit ( sample_one_chain , starter_seq , n_steps = 10 , scoring_func = scoring_func , pure = False # this is important, esp. with random sampling methods ) ) # Gather results from distributed workers chain_results = client . gather ( chain_results_futures ) # Convert everything into a single DataFrame chain_data = pd . concat ([ pd . DataFrame ( r ) for r in chain_results ]) Your contribution here Is there an \"advanced\" protocol that you've developed surrounding jax-unirep ? If so, please consider contributing it here!","title":"Advanced Usage"},{"location":"advanced/#advanced-usage","text":"APIs that support \"advanced\" tasks are available in jax-unirep . Read on to learn how to use them.","title":"Advanced Usage"},{"location":"advanced/#evotuning","text":"In the original paper the concept of 'evolutionary finetuning' is introduced, where the pre-trained mLSTM weights get fine-tuned through weight-updates using homolog protein sequences of a given protein of interest as input. This feature is available as well in jax-unirep . Given a set of starter weights for the mLSTM (defaults to the weights from the paper) as well as a set of sequences, the weights get fine-tuned in such a way that test set loss in the 'next-aa prediction task' is minimized. There are two functions with differing levels of control available. The evotune function uses optuna under the hood to automatically find: 1. the optimal number of epochs to train for, and 2. the optimal learning rate, given a set of sequences. The study object will contain all the information about the training process of each trial. evotuned_params will contain the fine-tuned mLSTM and dense weights from the trial with the lowest test set loss. Speed freaks read this! As a heads-up, using evotune is kind of slow, so read on if you're of the impatient kind -- use fit ! If you want to directly fine-tune the weights for a fixed number of epochs while using a fixed learning rate, you should use the fit function instead. The fit function has further customization options, such as different batching strategies. Please see the function docstring for more information. GPU usage The fit function will always default to using a GPU backend if available for the forward and backward passes during training of the LSTM. However, for the calulation of the average loss on the dataset after every epoch, you can decide if the CPU or GPU backend should be used (default is CPU). You can find an example usages of both evotune and fit here . If you want to pass a set of mLSTM and dense weights that were dumped in an earlier run, create params as follows: from jax_unirep.utils import load_params params = load_params ( folderpath = \"path/to/params/folder\" ) If you want to start from randomly initialized mLSTM and dense weights instead: from jax_unirep.evotuning import init_fun from jax.random import PRNGKey _ , params = init_fun ( PRNGKey ( 0 ), input_shape = ( - 1 , 10 )) The weights used in the 10-dimensional embedding of the input sequences always default to the weights from the paper, since they do not get updated during evotuning.","title":"Evotuning"},{"location":"advanced/#end-to-end-differentiable-models","text":"As a user, you might want to write custom \"top models\", such as a linear model on top of the reps, but might want to jointly optimize the UniRep weights with the top model reps. You're in luck! We implemented the mLSTM layers in such a way that they are compatible with jax.experimental.stax . This means that they can easily be plugged into a stax.serial model, e.g. to train both the mLSTM and a top-model at once: from jax.experimental import stax from jax.experimental.stax import Dense , Relu from jax_unirep.layers import mLSTM1900 , mLSTM1900_AvgHidden init_fun , apply_fun = stax . serial ( mLSTM1900 (), mLSTM1900_AvgHidden (), # Add two layers, one dense layer that results in 512-dim activations Dense ( 512 ), Relu (), # And then a linear layer to produce a 1-dim activation Dense ( 1 ) ) Have a look at the documentation and examples for more information about how to implement a model in jax .","title":"End-to-end differentiable models"},{"location":"advanced/#sampling-new-protein-sequences","text":"When doing protein engineering, one core task is proposing new sequences to order by gene synthesis. jax-unirep provides a number of utility functions inside jax_unirep.sampler that help with this task.","title":"Sampling new protein sequences"},{"location":"advanced/#basic-sampling","text":"The key one to focus on is the sample_one_chain function. This function takes in a starting sequence, and uses Monte Carlo sampling alongside the Metropolis-Hastings criteria to score and rank-order new sequences to try out. The usage pattern is as follows. Firstly, you must have a scoring function defined that takes in a string sequence, and outputs a number. This can be, for example, in the form of a pre-trained machine learning model that you have created. from jax_unirep import get_reps model = SomeSKLearnModel () model . fit ( training_X , training_y ) def scoring_func ( sequence : str ): reps , _ , _ = get_reps ( sequence ) return model . predict ( reps ) Now, we can use MCMC sampling to propose new sequences. from jax_unirep import sample_one_chain starter_seq = \"MKLNEQLJLA\" # can be longer! sampled_sequences = sample_one_chain ( starter_seq , n_steps = 10 , scoring_func = scoring_func ) sampled_seqs_df = pd . DataFrame ( sampled_sequences ) sampled_sequences is a dictionary that can be converted directly into a pandas.DataFrame . In there, every single sequence that was ever sampled is recorded, as well as its score (given by the scoring function) and whether it was accepted by the MCMC sampler or not. (All generated sequences are recorded, just in case there was something good that was rejected!)","title":"Basic sampling"},{"location":"advanced/#parallel-sampling","text":"If you want to do parallel sampling, you can use any library that does parallel processing. We're going to show you one example using Dask , which happens to be out favourite library for scalable Python! Assuming you have a Dask client object instantiated: client = Client ( ... ) # you'll have to configure this according to your own circumstances starter_seq = \"MKLNEQLJLA\" # can be longer! chain_results_futures = [] for i in range ( 100 ): # sample 100 independent chains chain_results_futures . append ( # Submit tasks to workers client . submit ( sample_one_chain , starter_seq , n_steps = 10 , scoring_func = scoring_func , pure = False # this is important, esp. with random sampling methods ) ) # Gather results from distributed workers chain_results = client . gather ( chain_results_futures ) # Convert everything into a single DataFrame chain_data = pd . concat ([ pd . DataFrame ( r ) for r in chain_results ])","title":"Parallel sampling"},{"location":"advanced/#your-contribution-here","text":"Is there an \"advanced\" protocol that you've developed surrounding jax-unirep ? If so, please consider contributing it here!","title":"Your contribution here"},{"location":"api/","text":"API Documentation Here lies the official top-level API for interacting with jax-unirep . Calculating Representations jax_unirep. get_reps ( seqs , params=None ) Get reps of proteins. This function generates representations of protein sequences using the 1900 hidden-unit mLSTM model with pre-trained weights from the UniRep paper . Each element of the output 3-tuple is a np.array of shape (n_input_sequences, 1900): h_avg : Average hidden state of the mLSTM over the whole sequence. h_final : Final hidden state of the mLSTM c_final : Final cell state of the mLSTM You should not use this function if you want to do further JAX-based computations on the output vectors! In that case, the DeviceArray futures returned by mLSTM1900 should be passed directly into the next step instead of converting them to np.array s. The conversion to np.array s is done in the dispatched rep_x_lengths functions to force python to wait with returning the values until the computation is completed. The keys of the params dictionary must be: b, gh, gmh, gmx, gx, wh, wmh, wmx, wx Parameters seqs: A list of sequences as strings or a single string. params: A dictionary of mLSTM1900 weights. Returns A 3-tuple of np.array s containing the reps, in the order h_avg , h_final , and c_final . Each np.array has shape (n_sequences, 1900). Evotuning jax_unirep. fit ( params , sequences , n_epochs , batch_method='random' , batch_size=25 , step_size=0.0001 , holdout_seqs=None , proj_name='temp' , epochs_per_print=1 , backend='cpu' ) Return mLSTM weights fitted to predict the next letter in each AA sequence. The training loop is as follows, depending on the batching strategy: Length batching: At each iteration, of all sequence lengths present in sequences , one length gets chosen at random. Next, batch_size number of sequences of the chosen length get selected at random. If there are less sequences of a given length than batch_size , all sequences of that length get chosen. Those sequences then get passed through the model. No padding of sequences occurs. To get batching of sequences by length done, we call on batch_sequences from our utils.py module, which returns a list of sub-lists, in which each sub-list contains the indices in the original list of sequences that are of a particular length. Random batching: Before training, all sequences get padded to be the same length as the longest sequence in sequences . Then, at each iteration, we randomly sample batch_size sequences and pass them through the model. The training loop does not adhere to the common notion of epochs , where all sequences would be seen by the model exactly once per epoch. Instead sequences always get sampled at random, and one epoch approximately consists of round(len(sequences) / batch_size) weight updates. Asymptotically, this should be approximately equiavlent to doing epoch passes over the dataset. To learn more about the passing of params , have a look at the evotune function docstring. You can optionally dump parameters and print weights every epochs_per_print epochs to monitor training progress. For ergonomics, training/holdout set losses are estimated on a batch size the same as batch_size , rather than calculated exactly on the entire set. Set epochs_per_print to None to avoid parameter dumping. Parameters params: mLSTM1900 and Dense parameters. sequences: List of sequences to evotune on. n: The number of iterations to evotune on. batch_method: One of \"length\" or \"random\". batch_size: If random batching is used, number of sequences per batch. As a rule of thumb, batch size of 50 consumes about 5GB of GPU RAM. step_size: The learning rate. holdout_seqs: Holdout set, an optional input. proj_name: The directory path for weights to be output to. epochs_per_print: Number of epochs to progress before printing and dumping of weights. Must be greater than or equal to 1. backend: Whether or not to use the GPU. Defaults to \"cpu\", but can be set to \"gpu\" if desired. Returns Final optimized parameters. Sampling jax_unirep. sample_one_chain ( starter_sequence , n_steps , scoring_func , is_accepted_kwargs={} , trust_radius=7 , propose_kwargs={} ) Return one chain of MCMC samples of new sequences. Given a starter_sequence , this function will sample one chain of protein sequences, scored using a user-provided scoring_func . Design choices made here include the following. Firstly, we record all sequences that were sampled, and not just the accepted ones. This behaviour differs from other MCMC samplers that record only the accepted values. We do this just in case sequences that are still \"good\" (but not better than current) are rejected. The effect here is that we get a cluster of sequences that are one-apart from newly accepted sequences. Secondly, we check the Hamming distance between the newly proposed sequences and the original. This corresponds to the \"trust radius\" specified in the jax-unirep paper . If the hamming distance > trust radius, we reject the sequence outright. A dictionary containing the following key-value pairs are returned: \"sequences\": All proposed sequences. \"scores\": All scores from the scoring function. \"accept\": Whether the sequence was accepted as the new 'current sequence' on which new sequences are proposed. This can be turned into a pandas DataFrame. Parameters starter_sequence: The starting sequence. n_steps: Number of steps for the MC chain to walk. scoring_func: Scoring function for a new sequence. It should only accept a string sequence . is_accepted_kwargs: Dictionary of kwargs to pass into is_accepted function. See is_accepted docstring for more details. trust_radius: Maximum allowed number of mutations away from starter sequence. propose_kwargs: Dictionary of kwargs to pass into propose function. See propose docstring for more details. verbose: Whether or not to print iteration number and associated sequence + score. Defaults to False Returns A dictionary with sequences , accept and score as keys.","title":"API Docs"},{"location":"api/#api-documentation","text":"Here lies the official top-level API for interacting with jax-unirep .","title":"API Documentation"},{"location":"api/#calculating-representations","text":"jax_unirep. get_reps ( seqs , params=None ) Get reps of proteins. This function generates representations of protein sequences using the 1900 hidden-unit mLSTM model with pre-trained weights from the UniRep paper . Each element of the output 3-tuple is a np.array of shape (n_input_sequences, 1900): h_avg : Average hidden state of the mLSTM over the whole sequence. h_final : Final hidden state of the mLSTM c_final : Final cell state of the mLSTM You should not use this function if you want to do further JAX-based computations on the output vectors! In that case, the DeviceArray futures returned by mLSTM1900 should be passed directly into the next step instead of converting them to np.array s. The conversion to np.array s is done in the dispatched rep_x_lengths functions to force python to wait with returning the values until the computation is completed. The keys of the params dictionary must be: b, gh, gmh, gmx, gx, wh, wmh, wmx, wx","title":"Calculating Representations"},{"location":"api/#evotuning","text":"jax_unirep. fit ( params , sequences , n_epochs , batch_method='random' , batch_size=25 , step_size=0.0001 , holdout_seqs=None , proj_name='temp' , epochs_per_print=1 , backend='cpu' ) Return mLSTM weights fitted to predict the next letter in each AA sequence. The training loop is as follows, depending on the batching strategy: Length batching: At each iteration, of all sequence lengths present in sequences , one length gets chosen at random. Next, batch_size number of sequences of the chosen length get selected at random. If there are less sequences of a given length than batch_size , all sequences of that length get chosen. Those sequences then get passed through the model. No padding of sequences occurs. To get batching of sequences by length done, we call on batch_sequences from our utils.py module, which returns a list of sub-lists, in which each sub-list contains the indices in the original list of sequences that are of a particular length. Random batching: Before training, all sequences get padded to be the same length as the longest sequence in sequences . Then, at each iteration, we randomly sample batch_size sequences and pass them through the model. The training loop does not adhere to the common notion of epochs , where all sequences would be seen by the model exactly once per epoch. Instead sequences always get sampled at random, and one epoch approximately consists of round(len(sequences) / batch_size) weight updates. Asymptotically, this should be approximately equiavlent to doing epoch passes over the dataset. To learn more about the passing of params , have a look at the evotune function docstring. You can optionally dump parameters and print weights every epochs_per_print epochs to monitor training progress. For ergonomics, training/holdout set losses are estimated on a batch size the same as batch_size , rather than calculated exactly on the entire set. Set epochs_per_print to None to avoid parameter dumping.","title":"Evotuning"},{"location":"api/#sampling","text":"jax_unirep. sample_one_chain ( starter_sequence , n_steps , scoring_func , is_accepted_kwargs={} , trust_radius=7 , propose_kwargs={} ) Return one chain of MCMC samples of new sequences. Given a starter_sequence , this function will sample one chain of protein sequences, scored using a user-provided scoring_func . Design choices made here include the following. Firstly, we record all sequences that were sampled, and not just the accepted ones. This behaviour differs from other MCMC samplers that record only the accepted values. We do this just in case sequences that are still \"good\" (but not better than current) are rejected. The effect here is that we get a cluster of sequences that are one-apart from newly accepted sequences. Secondly, we check the Hamming distance between the newly proposed sequences and the original. This corresponds to the \"trust radius\" specified in the jax-unirep paper . If the hamming distance > trust radius, we reject the sequence outright. A dictionary containing the following key-value pairs are returned: \"sequences\": All proposed sequences. \"scores\": All scores from the scoring function. \"accept\": Whether the sequence was accepted as the new 'current sequence' on which new sequences are proposed. This can be turned into a pandas DataFrame.","title":"Sampling"},{"location":"contributing/","text":"Contributing This page will show you how to get involved with contributing to the development of jax-unirep . Bug Reports Bug reports are definitely welcome on the issue tracker ! In filing a bug report, please produce a reproducible example. The standard for a reproducible example is that anyone with jax-unirep installed can copy/paste the code with no modifications into a Jupyter notebook (or other interpreter) and execute the code with no modifications. Doing this helps a ton with debugging and reduces maintainer friction in trying to help with debugging. If we are able to hit the root of the problem, we might post a possible solution and encourage you to submit a pull request so that you can partake in open source software co-creation with us. It's fun, and we're really sure you'd like it too ;). Documentation As maintainers of the package, we're inevitably going to have blind spots. If you find one, and think others would benefit from that blind spot being eliminated in the docs, please submit a pull request! All docs are housed in the docs/ directory, and are plain Markdown files, which should make editing easy, whether on GitHub or locally. Feature Requests Feature requests are always welcome to be posted on the issue tracker ! That said, please temper your expectations, as jax-unirep development happens as and when the lead maintainers (Arkadij Kummer and Eric Ma) encounter needs in their day jobs. We welcome your pull requests, and are happy to guide you through the development process and work with you to get what you need into the library, but any requests for us to implement features will be prioritized according to what we encounter in our day jobs. If you make in a pull request that gets accepted, we are more than happy to publicly acknowledge your contributions and by sending tons of positive vibes throughout the Twitterverse and our LinkedIn connections!","title":"Contributing"},{"location":"contributing/#contributing","text":"This page will show you how to get involved with contributing to the development of jax-unirep .","title":"Contributing"},{"location":"contributing/#bug-reports","text":"Bug reports are definitely welcome on the issue tracker ! In filing a bug report, please produce a reproducible example. The standard for a reproducible example is that anyone with jax-unirep installed can copy/paste the code with no modifications into a Jupyter notebook (or other interpreter) and execute the code with no modifications. Doing this helps a ton with debugging and reduces maintainer friction in trying to help with debugging. If we are able to hit the root of the problem, we might post a possible solution and encourage you to submit a pull request so that you can partake in open source software co-creation with us. It's fun, and we're really sure you'd like it too ;).","title":"Bug Reports"},{"location":"contributing/#documentation","text":"As maintainers of the package, we're inevitably going to have blind spots. If you find one, and think others would benefit from that blind spot being eliminated in the docs, please submit a pull request! All docs are housed in the docs/ directory, and are plain Markdown files, which should make editing easy, whether on GitHub or locally.","title":"Documentation"},{"location":"contributing/#feature-requests","text":"Feature requests are always welcome to be posted on the issue tracker ! That said, please temper your expectations, as jax-unirep development happens as and when the lead maintainers (Arkadij Kummer and Eric Ma) encounter needs in their day jobs. We welcome your pull requests, and are happy to guide you through the development process and work with you to get what you need into the library, but any requests for us to implement features will be prioritized according to what we encounter in our day jobs. If you make in a pull request that gets accepted, we are more than happy to publicly acknowledge your contributions and by sending tons of positive vibes throughout the Twitterverse and our LinkedIn connections!","title":"Feature Requests"},{"location":"development/","text":"Development This page will show you the officially-supported ways of getting set up with a development environment so that you can hack on jax-unirep and make contributions! Get familiar with community practices Kevin Markham has a great resource on how to contribute to open source software on GitHub. We'd really encourage you to look through it first if you're not already familiar with canonical, community workflow practices that have been adopted across multiple open source projects. The most basic ideas that you'll need to grasp are: Making forks. Local vs. remote. VSCode Dev Containers The easiest way for you to get setup is to use a dev container with VSCode. (We're not paid by Microsoft, we're just fans of this way of working.) To get started: Fork the repository. Ensure you have Docker running on your local machine. Ensure you have VSCode running on your local machine. In Visual Studio Code, click on the quick actions Status Bar item in the lower left corner. Then select \u201cRemote Containers: Open Repository In Container\u201d. Enter in the URL of your fork of pyjanitor. VSCode will pull down the prebuilt Docker container, git clone the repository for you inside an isolated Docker volume, and mount the repository directory inside your Docker container. Follow best practices to submit a pull request by making a feature branch. Now, hack away, and submit in your pull request! You shouln\u2019t be able to access the cloned repo on your local hard drive. If you do want local access, then clone the repo locally first before selecting \u201cRemote Containers: Open Folder In Container\u201d. If you find something is broken because a utility is missing in the container, submit a PR with the appropriate build command inserted in the Dockerfile. Care has been taken to document what each step does, so please read the in-line documentation in the Dockerfile carefully. Conda Environment This is another supported way of working. We assume that you already have the Anaconda distribution of Python setup on your local machine. Once you've done that: Fork the repository. Clone your fork locally. In your terminal, enter into the local copy of the repository. Now, install the environment: conda env create -f environment.yml This will create an environment called jax-unirep that you can activate. conda activate jax-unirep Finally, install jax-unirep into your environment in development mode. python setup.py develop Your favourite way here If you've got well-documented steps for how to get setup, come contribute them as part of the docs here! You'll want to edit docs/development.md . Then submit a pull request in: everyone in the community will benefit!","title":"Development"},{"location":"development/#development","text":"This page will show you the officially-supported ways of getting set up with a development environment so that you can hack on jax-unirep and make contributions!","title":"Development"},{"location":"development/#get-familiar-with-community-practices","text":"Kevin Markham has a great resource on how to contribute to open source software on GitHub. We'd really encourage you to look through it first if you're not already familiar with canonical, community workflow practices that have been adopted across multiple open source projects. The most basic ideas that you'll need to grasp are: Making forks. Local vs. remote.","title":"Get familiar with community practices"},{"location":"development/#vscode-dev-containers","text":"The easiest way for you to get setup is to use a dev container with VSCode. (We're not paid by Microsoft, we're just fans of this way of working.) To get started: Fork the repository. Ensure you have Docker running on your local machine. Ensure you have VSCode running on your local machine. In Visual Studio Code, click on the quick actions Status Bar item in the lower left corner. Then select \u201cRemote Containers: Open Repository In Container\u201d. Enter in the URL of your fork of pyjanitor. VSCode will pull down the prebuilt Docker container, git clone the repository for you inside an isolated Docker volume, and mount the repository directory inside your Docker container. Follow best practices to submit a pull request by making a feature branch. Now, hack away, and submit in your pull request! You shouln\u2019t be able to access the cloned repo on your local hard drive. If you do want local access, then clone the repo locally first before selecting \u201cRemote Containers: Open Folder In Container\u201d. If you find something is broken because a utility is missing in the container, submit a PR with the appropriate build command inserted in the Dockerfile. Care has been taken to document what each step does, so please read the in-line documentation in the Dockerfile carefully.","title":"VSCode Dev Containers"},{"location":"development/#conda-environment","text":"This is another supported way of working. We assume that you already have the Anaconda distribution of Python setup on your local machine. Once you've done that: Fork the repository. Clone your fork locally. In your terminal, enter into the local copy of the repository. Now, install the environment: conda env create -f environment.yml This will create an environment called jax-unirep that you can activate. conda activate jax-unirep Finally, install jax-unirep into your environment in development mode. python setup.py develop","title":"Conda Environment"},{"location":"development/#your-favourite-way-here","text":"If you've got well-documented steps for how to get setup, come contribute them as part of the docs here! You'll want to edit docs/development.md . Then submit a pull request in: everyone in the community will benefit!","title":"Your favourite way here"},{"location":"getting-started/","text":"Getting Started Installation Ensure that your compute environment allows you to run JAX code. (A modern Linux or macOS with a GLIBC>=2.23 is probably necessary.) For now, jax-unirep is available by pip installing from source. Installation for the moment is done from GitHub: pip install git+https://github.com/ElArkk/jax-unirep.git On the roadmap is support for installation from PyPI and conda-forge. Basic Usage The core activity with using UniRep is to produce fixed-length representations of protein sequences. This is done by using the get_reps() function. You can \"rep\" a single sequence: from jax_unirep import get_reps sequence = \"ASDFGHJKL\" # h_avg is the canonical \"reps\" h_avg , h_final , c_final = get_reps ( sequence ) Or you can \"rep\" a bunch of sequences together: from jax_unirep import get_reps sequences = [ \"ASDF\" , \"YJKAL\" , \"QQLAMEHALQP\" ] # h_avg is the canonical \"reps\" h_avg , h_final , c_final = get_reps ( sequences ) # each of the arrays will be of shape (len(sequences), 1900), # with the correct order of sequences preserved Canonically, you would use h_avg as the \"reps\".","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#installation","text":"Ensure that your compute environment allows you to run JAX code. (A modern Linux or macOS with a GLIBC>=2.23 is probably necessary.) For now, jax-unirep is available by pip installing from source. Installation for the moment is done from GitHub: pip install git+https://github.com/ElArkk/jax-unirep.git On the roadmap is support for installation from PyPI and conda-forge.","title":"Installation"},{"location":"getting-started/#basic-usage","text":"The core activity with using UniRep is to produce fixed-length representations of protein sequences. This is done by using the get_reps() function. You can \"rep\" a single sequence: from jax_unirep import get_reps sequence = \"ASDFGHJKL\" # h_avg is the canonical \"reps\" h_avg , h_final , c_final = get_reps ( sequence ) Or you can \"rep\" a bunch of sequences together: from jax_unirep import get_reps sequences = [ \"ASDF\" , \"YJKAL\" , \"QQLAMEHALQP\" ] # h_avg is the canonical \"reps\" h_avg , h_final , c_final = get_reps ( sequences ) # each of the arrays will be of shape (len(sequences), 1900), # with the correct order of sequences preserved Canonically, you would use h_avg as the \"reps\".","title":"Basic Usage"}]}