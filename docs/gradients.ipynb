{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients w.r.t. input\n",
    "\n",
    "For input design, \n",
    "i.e. designing a protein sequence to maximize (or minimize) \n",
    "the output of a neural network,\n",
    "one strategy is to perform gradient ascent/descent on the inputs.\n",
    "To do so, we need to take the gradient of the neural network output w.r.t. the input.\n",
    "At its core, this is taking the Jacobian the neural network;\n",
    "put simply, the Jacobian is a generalized form of the first derivative,\n",
    "while the Hessian, correspondingly, is the second derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_unirep.evotuning_models import mlstm256\n",
    "from jax.random import PRNGKey\n",
    "from jax_unirep.utils import seq_to_oh\n",
    "\n",
    "init_func, model_func = mlstm256()\n",
    "_, params = init_func(PRNGKey(42), input_shape=(-1, 26))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Label\n",
    "\n",
    "To begin, we need soft labels rather than one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np \n",
    "from jax import vmap\n",
    "\n",
    "def normalize_probability(v):\n",
    "    \"\"\"Normalize a vector to sum to 1.\"\"\"\n",
    "    return v / np.sum(v)\n",
    "\n",
    "def soft_label(v: np.ndarray, delta: float = 1e-2) -> np.ndarray:\n",
    "    \"\"\"Apply a soft label transformation on a vector `v`.\n",
    "    \n",
    "    This function assumes that `v` is a one-hot encoding vector.\n",
    "    \"\"\"\n",
    "    return normalize_probability(v + delta)\n",
    "\n",
    "soft_label_protein = vmap(soft_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "\n",
    "sequences = [\"HAPPYNEWYEAK\", \"HAPPYNEWYEAR\"]\n",
    "sequences = [soft_label_protein(seq_to_oh(i)) for i in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_unirep.layers import AAEmbedding, mLSTM, mLSTMHiddenStates, mLSTMAvgHidden\n",
    "from jax.example_libraries.stax import Dense, Softmax, serial, Relu\n",
    "\n",
    "model_layers = (\n",
    "    AAEmbedding(20),\n",
    "    mLSTM(512),\n",
    "    mLSTMHiddenStates(),\n",
    "    mLSTM(512),\n",
    "    mLSTMAvgHidden(),\n",
    "    Dense(1),\n",
    "    Relu,\n",
    ")\n",
    "\n",
    "init_fun, apply_fun = serial(*model_layers)\n",
    "\n",
    "_, params = init_fun(PRNGKey(42), input_shape=(-1, 26))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd\n",
    "apply_fun(params, sequences[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "\n",
    "jacfwd(partial(apply_fun, params))(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "745b07973f0370771e2c9122796b2e8f8679e70226d60fa1e11fde463ad0da41"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('jax-unirep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
