{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we are going to use the `fit` function to train a UniRep model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Here are the imports that we are going to need for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey\n",
    "from jax.example_libraries.stax import Dense, Softmax, serial\n",
    "\n",
    "from jax_unirep import fit\n",
    "from jax_unirep.evotuning_models import mlstm64\n",
    "from jax_unirep.utils import load_params\n",
    "from jax_unirep.layers import AAEmbedding, mLSTM, mLSTMHiddenStates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences\n",
    "\n",
    "We'll prepare a bunch of dummy sequences. \n",
    "\n",
    "In your _actual_ use case, you'll probably need to find a way to load your sequences into memory as a **list of strings**. (We try our best to stick with Python idioms.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"HASTA\", \"VISTA\", \"ALAVA\", \"LIMED\", \"HAST\", \"HAS\", \"HASVASTA\"] * 5\n",
    "holdout_sequences = [\n",
    "    \"HASTA\",\n",
    "    \"VISTA\",\n",
    "    \"ALAVA\",\n",
    "    \"LIMED\",\n",
    "    \"HAST\",\n",
    "    \"HASVALTA\",\n",
    "] * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Default mLSTM model\n",
    "\n",
    "In this first example, we'll use a default mLSTM1900 model with the shipped weights that are provided.\n",
    "\n",
    "Nothing needs to be passed in except for:\n",
    "\n",
    "1. The sequences to evotune against, and\n",
    "2. The number of epochs.\n",
    "\n",
    "It's the easiest/fastest way to get up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First way: Use the default mLSTM1900 weights with mLSTM1900 model.\n",
    "\n",
    "tuned_params = fit(sequences, n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Pre-build model architectures\n",
    "\n",
    "The second way is to use one of the pre-built evotuning models.\n",
    "The pre-trained weights for the three model architectures from the paper are shipped with the repo (1900, 256, 64).\n",
    "You can also leverage JAX to reproducibly initialize random parameters.\n",
    "\n",
    "In this example, we'll use the `mlstm64` model.\n",
    "The `mlstm256` model is also available,\n",
    "and it might give you better performance\n",
    "though at the price of longer training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fun, apply_fun = mlstm64()\n",
    "\n",
    "# The init_func always requires a PRNGKey,\n",
    "# and input_shape should be set to (-1, 26)\n",
    "# This creates randomly initialized parameters\n",
    "_, params = init_fun(PRNGKey(42), input_shape=(-1, 26))\n",
    "\n",
    "# Alternatively, you can load the paper weights\n",
    "params = load_params(paper_weights=64)\n",
    "\n",
    "\n",
    "# Now we tune the params.\n",
    "tuned_params = fit(sequences, n_epochs=2, model_func=apply_fun, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Build your own model\n",
    "\n",
    "Finally, the modular style of `jax-unirep` allows you to easily try out your own model architectures. You could for example change the amount of inital embedding dimensions, or the mLSTM architecture. Let's try a model with 20 inital embedding dimensions instead of 10, and two stacked mLSTM's with 512 hidden states each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layers = (\n",
    "        AAEmbedding(20),\n",
    "        mLSTM(512),\n",
    "        mLSTMHiddenStates(),\n",
    "        mLSTM(512),\n",
    "        mLSTMHiddenStates(),\n",
    "        Dense(25),\n",
    "        Softmax,\n",
    "    )\n",
    "\n",
    "init_fun, apply_fun = serial(*model_layers)\n",
    "\n",
    "_, params = init_fun(PRNGKey(42), input_shape=(-1, 26))\n",
    "\n",
    "tuned_params = fit(sequences, n_epochs=2, model_func=apply_fun, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obviously...\n",
    "\n",
    "...you would probably swap in/out a different set of sequences and train for longer :)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-unirep",
   "language": "python",
   "name": "jax-unirep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
