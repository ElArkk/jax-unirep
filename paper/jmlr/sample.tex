\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{meila00a}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

\ShortHeadings{jax-unirep: A user-centered, performant reimplentation of UniRep in JAX}{Kummer, Jayapurna and Ma}
\firstpageno{1}

\begin{document}

\title{jax-unirep: Accelerated and User-Friendly Protein Machine Learning with JAX}

\author{\name Arkadij Kummer \email arkadij.kummer@novartis.com \\
       \addr Global Discovery Chemistry\\
       Novartis Institutes for Biomedical Research\\
       Basel, Switzerland
       \AND
       \name Ivan Jayapurna \email ivanfj@berkeley.edu \\
       \addr University of California\\
       University of California\\
       Berkeley, CA, USA
       \AND
       \name Eric J. Ma \email eric.ma@novartis.com \\
       \addr NIBR Informatics\\
       Novartis Institutes for Biomedical Research\\
       Cambridge, MA, USA
       }

\editor{Kevin Murphy and Bernhard Sch{\"o}lkopf}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
We provide a significantly upgraded version of UniRep, jax-unirep,
a recurrent neural network model trained on 24 million protein sequences,
with significant speed improvements,
API enhancements,
and reliability upgrades.
jax-unirep is implemented in JAX,
and takes advantage of JAX's programming model
to provide up to 100X speedup over the original implementation.
Surrounding the package are API enhancements
that support the "machine learning in protein engineering" workflow,
including proposing new sequences to sample,
writing end-to-end differentiable models with JAX
that involve UniRep as a component
and weight tuning,
all of which require minimal user intervention.
jax-unirep has easily accessible documentation,
extensive (>93\%) code test coverage,
automatic testing and docs redeployment,
readily copy-pasteable examples for specific workflow pieces.
\end{abstract}

\begin{keywords}
  Recurrent Neural Networks, Machine Learning Application, Protein Engineering
\end{keywords}

\section{Introduction}

UniRep is a recurrent neural network,
trained using self-supervision
on 24 million protein sequences
to predict the next amino acid in a sequence (\cite{alley2019unified}).
Its most powerful model allows for embedding
arbitrary length sequences in a 1900-long feature vector
that can be used as the input to a downstream model
for unsupervised clustering or supervised prediction of protein properties.

The original model (also referred to here as \verb|tf-unirep|)
was implemented in TensorFlow 1.13 (\cite{abadi2016tensorflow}).
While test-driving \verb|tf-unirep|, we observed two issues
that hindered us from using it productively.
The first is that the original implementation
took an abnormally long amount of time to process multiple sequences,
requiring on the order of dozens of seconds to process tens of sequences.
The second was that its API was unwieldy,
and could have been simplified for protein engineers
who might not necessarily be programming-savvy.

In the presented library, we present a significant set of upgrades
over the original implementation.
Firstly, we accelerated the model 100X over the original,
and provide a benchmark comparison on the most common use case for UniRep.
Secondly, we added a library of APIs
that ease the usage of the model in protein engineering workflows,
as well as neural network layer definitions
that allow its inclusion in end-to-end differentiable models.
Thirdly, we provide extensive software testing,
much beyond what was provided in the original.
Finally, we provide documentation for the model,
including introductory and advanced workflows,
and contribution instructions.

\section{Speed Profiling of UniRep}

\begin{figure*}[!tpb]
    \centerline{\includegraphics[width=6in]{fig01.jpg}}
    \caption{
        A. Speed comparison between the original implementation (UniRep)
        and our re-implementation (jax-unirep).
        Both one and ten random sequences of length ten
        were transformed by both implementations.
        B. Comparison of the average hidden state between the implementations
        when transforming the same sequence.
        The inset shows a slice of length 50 out of the total 1900 dimensions.
    }
    \label{fig:01}
\end{figure*}

To investigate the performance of \verb|tf-unirep|,
we used Python's \verb|cProfile| facility to identify speed bottlenecks.
On the basis of this, we hypothesized that the cause of speed problems
was graph compilation in TF1.x,
and that we could obtain speedups by using
a non-graph-compiled tensor library.

There were three options available: TF2.x, PyTorch (\cite{pytorch})
and JAX (\cite{jax2018github}).
We chose JAX, because it provides automatic differentiation
on top of the idiomatic NumPy API (\cite{oliphant2006guide}).
Using a familiar API with additional performant primitive functions,
e.g. \verb|lax.scan|, \verb|jit| and \verb|vmap|
reduces the barrier to community contributions.


\section{Architecture and Design}

In this section, we detail the architecture and design
of \verb|jax-unirep|.

\subsection{Top-level API}

\verb|jax-unirep|'s top-level API provides direct access
to user-facing functions:

\begin{itemize}
	\item \verb|get_reps| for a continuous, 1900-long representation of a protein sequence,
	\item \verb|fit| and \verb|evotune| for "one-touch" evolutionary tuning of the model,
	\item \verb|sample_one_chain| to perform \it{in silico} scoring of newly suggested mutants.
\end{itemize}

These cover the majority of tasks in a computational protein engineer's workflow
that will also involve a machine learning model for scoring of mutants.

\subsection{Model Implementation}

We treat the core model implementation as a function
that accepts a dictionary of parameters and input data.
That core function is in a sufficiently generic form (Python function)
that it can be easily incorporated into neural network layers.
As a demonstration, we wrote \verb|stax|-compatible layers,
though in principle the layers can be made Keras- or PyTorch-compatible easily,
thanks to the core functions being written in generic Python and NumPy.
Because neural network layers are nothing more than math functions
composed into higher-order functions,
we believe this architecture facilitates maximal portability
across deep learning APIs.

\subsection{Acceleration with JAX}

In the reimplementation of the RNN,
we took advantage of JAX's \verb|lax.scan| function,
which allowed us to write performant RNN loops.
We also used \verb|vmap| for performant mapping of a function
over leading tensor axes
and \verb|jit| to leverage just-in-time compilation.

\section{Significant Software Improvements}

In this section,
we detail the software improvements of jax-unirep over the original.

\subsection{Speed enhancements}

In a formal speed comparison, we found that \verb|jax-unirep|
performed up to 100X faster on the task of computing protein representations
(using the \verb|get_reps()| function) (Figure 1A\vphantom{\ref{fig:01}}).
We did not test more than 10 sequences
as the original unirep implementation would have taken
an unreasonably long amount of time to run
when using the comparable \verb|babbler.get_reps()| class method.
Comparisons to manual preprocessing for multiple sequences were not done,
as it would not correspond to what we believe to be
the most natural usage pattern of a first-time user of the original model:
simply looping over sequences rather than perform custom preprocessing.
Speed comparisons were performed on a single core of a 24-core
CentOS7 workstation running Intel Xeon processors at 3.6 GHz
with at least 7 replicates.

\subsection{Processing multiple protein sequences}

Processing multiple sequences is one place where \verb|jax-unirep|
is more user-friendly than \verb|tf-unirep|.
In \verb|tf-unirep|, to process multiple sequences,
an end-user would call \verb|babbler.get_reps(sequence)| in a loop,
essentially repeatedly paying the TensorFlow graph compilation penalty.
Alternatively, they would have to manually preprocess sequences
into the tensorized embedding using custom code
before passing them into the UniRep model,
impeding reproducibility of model usage results,
and making comparisons between users extremely difficult.

With \verb|jax-unirep|,
to obtain numeric representations of single and multiple protein sequences,
end-users use the high-level function \verb|get_reps()|,
which we have designed to accept single sequences and multiple sequences.
In the case of multiple sequences, the sequences need not necessarily be
of the same length.
In each case, conversion from strings to tensors happens automatically,
eliminating onne source of user error affecting modelling results,
thereby guaranteeing consistency between model usage runs.

\subsection{Evolutionary tuning}

"Evotuning" is fine-tuning of the UniRep weights
for a defined collection
of evolutionarily-related protein sequences (\cite{alley2019unified}).
Here, we provide a \verb|fit()| function to perform this task,
in which most of the code necessary for preprocessing the sequences
and performing gradient updates to the weights
are abstracted away from the end-user.
Additionally, automatic checkpointing
into a user-specified directory is available,
allowing users to select
which "evotuned" weights along the optimization path
they wish to use.

\subsection{Software testing and documentation}

We also provide software tests and documentation,
two pieces sorely lacking in \verb|tf-unirep|.
In particular, our test suite covers over 93\% of the codebase;
not only are core tensor operations tested for shape correctness,
we have also unit- and integration-tested
the majority of user-facing functions.
As such, end-users can use the model reproducibly with confidence.

Additionally, documentation is provided on advanced workflow pieces
that can be modularly incorporated
into a computational protein engineer's workflow.
Starter code are provided that can be copy and pasted easily,
thus providing an easy-to-use starting point for end-users.

Finally, we welcome community contributions
from amongst the technically adept users.
One of the authors (IJ) found an earlier version of the package
and contributed by working out the evotuning API,
as well as providing self-contained examples to test execution times
on Google Colab's GPUs.
As such, documentation is provided to help newcomer contributors
get set up for contributing.

\section{Reimplementation Verification}

To check that \verb|jax-unirep| was implemented correctly,
we computed numeric representations for single protein sequences
using the original and the reimplementation.
Figure 1B\vphantom{\ref{fig:01}} shows, for one example sequence,
that the representations are identical;
the difference in the traces were injected after computation
merely to visually separate them.

\section{Discussion and Future Work}

Through our reimplementation of UniRep,
we have effectively provided it with non-trivial upgrades over the original.
Having verified its correctness in reimplementation,
we have improved the API, added software tests,
and sped up model forward pass speed by nearly 100X,
all of which are significant upgrades over the original implementation.

Significant effort was invested in accelerating, refactoring, documenting,
and testing the \verb|babbler1900| model.
This was the model with the largest "capacity" by number of parameters),
and likely is the most powerful of the three described in the original paper.
Future work includes implementing
the \verb|babbler256| and \verb|babbler64| models.

\acks{We would like to acknowledge the original UniRep authors
for open sourcing their implementation,
our colleagues in the Protein Engineering Platform at NIBR
for providing a use case for the model,
and our beta tester colleagues
in the Chemical Biology and Therapeutics platform at NIBR.
Finally, we would like to thank our users across the world
who have contributed their time raising issues and helping us find bugs.}
% Acknowledgements should go at the end, before appendices and references

\vskip 0.2in
\bibliography{sample}

\end{document}
